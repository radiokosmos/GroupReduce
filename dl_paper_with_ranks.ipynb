{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_paper (1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "NFdz5c7CxNyq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "import torch\n",
        "import torch.nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from lm import repackage_hidden, LM_LSTM\n",
        "import numpy as np\n",
        "import reader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oQ2Z7nR3mBVY"
      },
      "cell_type": "markdown",
      "source": [
        "Ниже - обучение базовой модели из статьи: bigLSTM(small PTB)\n",
        "Обученная модель сохранена в файле lm_model.pt  \n",
        "Поэтому это можно скипнуть"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4prWa3UzyNGg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "data = 'data'\n",
        "hidden_size = 200\n",
        "num_steps = 35\n",
        "num_layers = 2\n",
        "batch_size = 20\n",
        "num_epochs = 13 \n",
        "dp_keep_prob = 0.35\n",
        "inital_lr = 20.0\n",
        "save = 'lm_model.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oOZhDYxL6mAT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
        "  \n",
        "\n",
        "  if type(h) == torch.Tensor:\n",
        "    return Variable(h.data)\n",
        "  else:\n",
        "    d = tuple(repackage_hidden(v) for v in h)\n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xK-sZhMs2Tq8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_epoch(model, data, is_train=False, lr=1.0):\n",
        "    \"\"\"Runs the model on the given data.\"\"\"\n",
        "    if is_train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    epoch_size = ((len(data) // model.batch_size) - 1) // model.num_steps\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden()\n",
        "    hidden[0].requires_grad=True\n",
        "    hidden[1].requires_grad=True\n",
        "    costs = 0.0\n",
        "    iters = 0\n",
        "    for step, (x, y) in enumerate(reader.ptb_iterator(data, model.batch_size, model.num_steps)):\n",
        "        inputs =torch.from_numpy(x.astype(np.int64)).transpose(0, 1).contiguous().cuda()\n",
        "        model.zero_grad()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        targets = torch.from_numpy(y.astype(np.int64)).transpose(0, 1).contiguous().cuda()\n",
        "        tt = torch.squeeze(targets.view(-1, model.batch_size * model.num_steps))\n",
        "\n",
        "        loss = criterion(outputs.view(-1, model.vocab_size), tt)\n",
        "        #print( loss.data.item() , model.num_steps)\n",
        "        costs += loss.data.item() * model.num_steps\n",
        "        iters += model.num_steps\n",
        "\n",
        "        if is_train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)\n",
        "            for p in model.parameters():\n",
        "                p.data.add_(-lr, p.grad.data)\n",
        "            if step % (epoch_size // 10) == 10:\n",
        "                print(\"{} perplexity: {:8.2f} speed: {} wps\".format(step * 1.0 / epoch_size, np.exp(costs / iters),\n",
        "                                                       iters * model.batch_size / (time.time() - start_time)))\n",
        "    return np.exp(costs / iters)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cPpy_FSz0n3V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for google collab\n",
        "!mkdir /content/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tINPnduTz_tI",
        "outputId": "1e7111a5-24c6-4dc5-ac0f-9f87e3b6acff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2758
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "raw_data = reader.ptb_raw_data(data_path=data)\n",
        "train_data, valid_data, test_data, word_to_id, id_2_word = raw_data\n",
        "vocab_size = len(word_to_id)\n",
        "print('Vocabluary size: {}'.format(vocab_size))\n",
        "model = LM_LSTM(embedding_dim=hidden_size, num_steps=num_steps, batch_size=batch_size,\n",
        "                vocab_size=vocab_size, num_layers=num_layers, dp_keep_prob=dp_keep_prob)\n",
        "model.cuda()\n",
        "lr = inital_lr\n",
        "# decay factor for learning rate\n",
        "lr_decay_base = 1 / 1.15\n",
        "# we will not touch lr for the first m_flat_lr epochs\n",
        "m_flat_lr = 14.0\n",
        "\n",
        "print(\"########## Training ##########################\")\n",
        "for epoch in range(num_epochs):\n",
        "    lr_decay = lr_decay_base ** max(epoch - m_flat_lr, 0)\n",
        "    lr = lr * lr_decay # decay lr if it is time\n",
        "    train_p = run_epoch(model, train_data, True, lr)\n",
        "    print('Train perplexity at epoch {}: {:8.2f}'.format(epoch, train_p))\n",
        "    print('Validation perplexity at epoch {}: {:8.2f}'.format(epoch, run_epoch(model, valid_data)))\n",
        "print(\"########## Testing ##########################\")\n",
        "model.batch_size = 1 # to make sure we process all the data\n",
        "print('Test Perplexity: {:8.2f}'.format(run_epoch(model, test_data)))\n",
        "with open(save, 'wb') as f:\n",
        "    torch.save(model, f)\n",
        "print(\"########## Done! ##########################\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabluary size: 10000\n",
            "########## Training ##########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.007535795026375283 perplexity:  4893.80 speed: 15333.384672284516 wps\n",
            "0.10700828937452901 perplexity:  1129.30 speed: 27170.95643940926 wps\n",
            "0.20648078372268275 perplexity:   894.38 speed: 28152.11265621626 wps\n",
            "0.3059532780708365 perplexity:   763.39 speed: 28525.145137913034 wps\n",
            "0.4054257724189902 perplexity:   687.17 speed: 28735.60676872225 wps\n",
            "0.5048982667671439 perplexity:   633.69 speed: 28844.897095030483 wps\n",
            "0.6043707611152976 perplexity:   584.44 speed: 28917.652977779464 wps\n",
            "0.7038432554634514 perplexity:   547.92 speed: 28967.146064885612 wps\n",
            "0.8033157498116051 perplexity:   519.32 speed: 29012.09219877926 wps\n",
            "0.9027882441597589 perplexity:   493.47 speed: 29041.501072367202 wps\n",
            "Train perplexity at epoch 0:   473.16\n",
            "Validation perplexity at epoch 0:   281.79\n",
            "0.007535795026375283 perplexity:   371.09 speed: 23262.19686203725 wps\n",
            "0.10700828937452901 perplexity:   300.43 speed: 28737.658264237896 wps\n",
            "0.20648078372268275 perplexity:   309.71 speed: 29016.716740110794 wps\n",
            "0.3059532780708365 perplexity:   304.49 speed: 29099.437790905868 wps\n",
            "0.4054257724189902 perplexity:   302.89 speed: 29155.727418505743 wps\n",
            "0.5048982667671439 perplexity:   301.29 speed: 29187.015937173466 wps\n",
            "0.6043707611152976 perplexity:   295.11 speed: 29211.781261905217 wps\n",
            "0.7038432554634514 perplexity:   291.40 speed: 29228.338826940817 wps\n",
            "0.8033157498116051 perplexity:   288.25 speed: 29236.210581752897 wps\n",
            "0.9027882441597589 perplexity:   283.75 speed: 29242.76403233724 wps\n",
            "Train perplexity at epoch 1:   280.60\n",
            "Validation perplexity at epoch 1:   222.43\n",
            "0.007535795026375283 perplexity:   296.95 speed: 23575.322520481226 wps\n",
            "0.10700828937452901 perplexity:   243.20 speed: 28773.34492719933 wps\n",
            "0.20648078372268275 perplexity:   254.38 speed: 29016.73863917336 wps\n",
            "0.3059532780708365 perplexity:   251.33 speed: 29105.82178200844 wps\n",
            "0.4054257724189902 perplexity:   252.14 speed: 29158.975430125553 wps\n",
            "0.5048982667671439 perplexity:   252.43 speed: 29191.66944484703 wps\n",
            "0.6043707611152976 perplexity:   248.70 speed: 29214.60180971258 wps\n",
            "0.7038432554634514 perplexity:   247.12 speed: 29236.23857913156 wps\n",
            "0.8033157498116051 perplexity:   245.82 speed: 29246.705056616516 wps\n",
            "0.9027882441597589 perplexity:   242.93 speed: 29252.68549671049 wps\n",
            "Train perplexity at epoch 2:   241.27\n",
            "Validation perplexity at epoch 2:   197.18\n",
            "0.007535795026375283 perplexity:   267.74 speed: 23507.267233868486 wps\n",
            "0.10700828937452901 perplexity:   219.86 speed: 28827.215594015965 wps\n",
            "0.20648078372268275 perplexity:   231.37 speed: 29083.628358813035 wps\n",
            "0.3059532780708365 perplexity:   228.73 speed: 29179.13919140709 wps\n",
            "0.4054257724189902 perplexity:   229.68 speed: 29222.94389890811 wps\n",
            "0.5048982667671439 perplexity:   230.34 speed: 29236.12109702974 wps\n",
            "0.6043707611152976 perplexity:   227.22 speed: 29242.898572928836 wps\n",
            "0.7038432554634514 perplexity:   226.51 speed: 29251.94024649455 wps\n",
            "0.8033157498116051 perplexity:   225.80 speed: 29264.22335305131 wps\n",
            "0.9027882441597589 perplexity:   223.54 speed: 29273.324414159822 wps\n",
            "Train perplexity at epoch 3:   222.47\n",
            "Validation perplexity at epoch 3:   183.43\n",
            "0.007535795026375283 perplexity:   255.90 speed: 23440.624480597482 wps\n",
            "0.10700828937452901 perplexity:   206.09 speed: 28751.495040681824 wps\n",
            "0.20648078372268275 perplexity:   217.98 speed: 29016.335076042844 wps\n",
            "0.3059532780708365 perplexity:   215.51 speed: 29111.19438591342 wps\n",
            "0.4054257724189902 perplexity:   216.67 speed: 29171.023457894375 wps\n",
            "0.5048982667671439 perplexity:   217.78 speed: 29207.07032845298 wps\n",
            "0.6043707611152976 perplexity:   214.98 speed: 29219.907015332214 wps\n",
            "0.7038432554634514 perplexity:   214.52 speed: 29228.008025753108 wps\n",
            "0.8033157498116051 perplexity:   214.14 speed: 29220.994984685945 wps\n",
            "0.9027882441597589 perplexity:   212.19 speed: 29223.674886847148 wps\n",
            "Train perplexity at epoch 4:   211.32\n",
            "Validation perplexity at epoch 4:   173.99\n",
            "0.007535795026375283 perplexity:   244.26 speed: 23569.111579144024 wps\n",
            "0.10700828937452901 perplexity:   197.19 speed: 28809.543376958805 wps\n",
            "0.20648078372268275 perplexity:   209.04 speed: 29073.710717623482 wps\n",
            "0.3059532780708365 perplexity:   206.71 speed: 29329.163033524557 wps\n",
            "0.4054257724189902 perplexity:   208.00 speed: 29482.818094594015 wps\n",
            "0.5048982667671439 perplexity:   208.89 speed: 29576.135165412248 wps\n",
            "0.6043707611152976 perplexity:   206.39 speed: 29583.800690852182 wps\n",
            "0.7038432554634514 perplexity:   206.32 speed: 29552.893782335785 wps\n",
            "0.8033157498116051 perplexity:   206.09 speed: 29537.20339594631 wps\n",
            "0.9027882441597589 perplexity:   204.22 speed: 29517.737454456154 wps\n",
            "Train perplexity at epoch 5:   203.50\n",
            "Validation perplexity at epoch 5:   169.52\n",
            "0.007535795026375283 perplexity:   237.60 speed: 23222.471108656762 wps\n",
            "0.10700828937452901 perplexity:   190.77 speed: 28773.81424903707 wps\n",
            "0.20648078372268275 perplexity:   202.85 speed: 29064.653561207288 wps\n",
            "0.3059532780708365 perplexity:   200.74 speed: 29182.25462944206 wps\n",
            "0.4054257724189902 perplexity:   202.13 speed: 29226.76772292986 wps\n",
            "0.5048982667671439 perplexity:   203.20 speed: 29251.724856784094 wps\n",
            "0.6043707611152976 perplexity:   201.02 speed: 29275.44794651883 wps\n",
            "0.7038432554634514 perplexity:   200.88 speed: 29280.036296864415 wps\n",
            "0.8033157498116051 perplexity:   200.82 speed: 29295.62847099552 wps\n",
            "0.9027882441597589 perplexity:   198.96 speed: 29303.424529341937 wps\n",
            "Train perplexity at epoch 6:   198.42\n",
            "Validation perplexity at epoch 6:   162.96\n",
            "0.007535795026375283 perplexity:   230.61 speed: 23760.597573186602 wps\n",
            "0.10700828937452901 perplexity:   186.29 speed: 28861.14107174724 wps\n",
            "0.20648078372268275 perplexity:   198.61 speed: 29135.536452388253 wps\n",
            "0.3059532780708365 perplexity:   196.41 speed: 29214.37697083394 wps\n",
            "0.4054257724189902 perplexity:   197.73 speed: 29247.65367882032 wps\n",
            "0.5048982667671439 perplexity:   198.74 speed: 29277.00486740255 wps\n",
            "0.6043707611152976 perplexity:   196.40 speed: 29288.368579086484 wps\n",
            "0.7038432554634514 perplexity:   196.45 speed: 29310.315823586272 wps\n",
            "0.8033157498116051 perplexity:   196.35 speed: 29324.06407325259 wps\n",
            "0.9027882441597589 perplexity:   194.76 speed: 29328.632619859316 wps\n",
            "Train perplexity at epoch 7:   194.27\n",
            "Validation perplexity at epoch 7:   162.00\n",
            "0.007535795026375283 perplexity:   228.14 speed: 23825.295306667453 wps\n",
            "0.10700828937452901 perplexity:   182.60 speed: 28891.24928881462 wps\n",
            "0.20648078372268275 perplexity:   194.91 speed: 29132.469930165003 wps\n",
            "0.3059532780708365 perplexity:   192.97 speed: 29251.184228254162 wps\n",
            "0.4054257724189902 perplexity:   194.21 speed: 29398.629295795192 wps\n",
            "0.5048982667671439 perplexity:   195.41 speed: 29427.496354246923 wps\n",
            "0.6043707611152976 perplexity:   193.16 speed: 29416.183596686988 wps\n",
            "0.7038432554634514 perplexity:   193.16 speed: 29411.90679685979 wps\n",
            "0.8033157498116051 perplexity:   193.19 speed: 29409.493226301067 wps\n",
            "0.9027882441597589 perplexity:   191.59 speed: 29404.82259461013 wps\n",
            "Train perplexity at epoch 8:   191.18\n",
            "Validation perplexity at epoch 8:   158.97\n",
            "0.007535795026375283 perplexity:   221.83 speed: 23636.26309206583 wps\n",
            "0.10700828937452901 perplexity:   180.02 speed: 28823.14277213321 wps\n",
            "0.20648078372268275 perplexity:   192.48 speed: 29084.13227595887 wps\n",
            "0.3059532780708365 perplexity:   190.37 speed: 29159.958853458662 wps\n",
            "0.4054257724189902 perplexity:   191.51 speed: 29198.845279880952 wps\n",
            "0.5048982667671439 perplexity:   192.40 speed: 29221.786141658868 wps\n",
            "0.6043707611152976 perplexity:   190.48 speed: 29239.35780023708 wps\n",
            "0.7038432554634514 perplexity:   190.56 speed: 29259.097743682934 wps\n",
            "0.8033157498116051 perplexity:   190.54 speed: 29280.900524210472 wps\n",
            "0.9027882441597589 perplexity:   188.91 speed: 29288.979231193855 wps\n",
            "Train perplexity at epoch 9:   188.55\n",
            "Validation perplexity at epoch 9:   154.83\n",
            "0.007535795026375283 perplexity:   218.65 speed: 23553.40099242409 wps\n",
            "0.10700828937452901 perplexity:   177.31 speed: 28786.329885097348 wps\n",
            "0.20648078372268275 perplexity:   189.79 speed: 29058.012373678364 wps\n",
            "0.3059532780708365 perplexity:   187.27 speed: 29142.631599728524 wps\n",
            "0.4054257724189902 perplexity:   188.63 speed: 29175.117161024944 wps\n",
            "0.5048982667671439 perplexity:   189.85 speed: 29192.971914258815 wps\n",
            "0.6043707611152976 perplexity:   187.90 speed: 29213.600510802833 wps\n",
            "0.7038432554634514 perplexity:   188.12 speed: 29242.17195882913 wps\n",
            "0.8033157498116051 perplexity:   188.23 speed: 29260.44367406055 wps\n",
            "0.9027882441597589 perplexity:   186.63 speed: 29268.59881791016 wps\n",
            "Train perplexity at epoch 10:   186.33\n",
            "Validation perplexity at epoch 10:   154.56\n",
            "0.007535795026375283 perplexity:   219.87 speed: 23616.058498775183 wps\n",
            "0.10700828937452901 perplexity:   176.72 speed: 28798.98679107889 wps\n",
            "0.20648078372268275 perplexity:   188.72 speed: 29105.238083702752 wps\n",
            "0.3059532780708365 perplexity:   186.63 speed: 29166.127389660895 wps\n",
            "0.4054257724189902 perplexity:   187.88 speed: 29207.753480211362 wps\n",
            "0.5048982667671439 perplexity:   188.91 speed: 29237.741255215224 wps\n",
            "0.6043707611152976 perplexity:   186.88 speed: 29243.449912446464 wps\n",
            "0.7038432554634514 perplexity:   187.05 speed: 29257.84351781605 wps\n",
            "0.8033157498116051 perplexity:   187.25 speed: 29278.564285997538 wps\n",
            "0.9027882441597589 perplexity:   185.62 speed: 29282.689329714038 wps\n",
            "Train perplexity at epoch 11:   185.26\n",
            "Validation perplexity at epoch 11:   153.12\n",
            "0.007535795026375283 perplexity:   219.66 speed: 23515.123414541802 wps\n",
            "0.10700828937452901 perplexity:   175.15 speed: 28738.893605882793 wps\n",
            "0.20648078372268275 perplexity:   187.33 speed: 29035.265610927338 wps\n",
            "0.3059532780708365 perplexity:   185.22 speed: 29131.13508985985 wps\n",
            "0.4054257724189902 perplexity:   186.53 speed: 29176.752381935865 wps\n",
            "0.5048982667671439 perplexity:   187.74 speed: 29201.91281970266 wps\n",
            "0.6043707611152976 perplexity:   185.49 speed: 29226.520218505968 wps\n",
            "0.7038432554634514 perplexity:   185.64 speed: 29247.658397498977 wps\n",
            "0.8033157498116051 perplexity:   185.71 speed: 29262.52910080834 wps\n",
            "0.9027882441597589 perplexity:   183.96 speed: 29263.712730953524 wps\n",
            "Train perplexity at epoch 12:   183.57\n",
            "Validation perplexity at epoch 12:   152.34\n",
            "########## Testing ##########################\n",
            "Test Perplexity:   146.82\n",
            "########## Done! ##########################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WG-RPKlZmtnM"
      },
      "cell_type": "markdown",
      "source": [
        "Загрузка обученной модели"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "thTFxgR2T8jD",
        "outputId": "87a1a254-43de-4a30-ef80-8cbb884731cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "raw_data = reader.ptb_raw_data(data_path=data)\n",
        "train_data, valid_data, test_data, word_to_id, id_2_word = raw_data\n",
        "# vocab_size = len(word_to_id)\n",
        "# print('Vocabluary size: {}'.format(vocab_size))\n",
        "model =  torch.load('./lm_model.pt')\n",
        "model.cuda()\n",
        "model.eval()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LM_LSTM(\n",
              "  (dropout): Dropout(p=0.65)\n",
              "  (word_embeddings): Embedding(10000, 200)\n",
              "  (lstm): LSTM(200, 200, num_layers=2, dropout=0.65)\n",
              "  (sm_fc): Linear(in_features=200, out_features=10000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zOV2GEOjhKn_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "freq = {key:0 for key in id_2_word.keys()}\n",
        "for j in train_data:\n",
        "    freq[j] +=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "boaXGwroA_9l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "freq_sqr = np.array([sqrt(q) for key, q in freq.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qx8P53jR9yVl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weighted_svd(A, freq_sqr, rank_to_be): \n",
        "    Q = torch.diag(torch.from_numpy(freq_sqr).cuda().type(torch.cuda.FloatTensor))\n",
        "    QA = torch.matmul(Q, A)\n",
        "    U_, S_, V_ = torch.svd(QA)\n",
        "    #print(Q.shape, V_.shape, S_.shape)\n",
        "    #print(torch.matmul(torch.inverse(Q),U_).shape)\n",
        "    rank_to_be = min(torch.matrix_rank(A), rank_to_be)\n",
        "    S = torch.cat((S_[:rank_to_be], torch.zeros_like(S_[rank_to_be:])))\n",
        "    U = torch.matmul(torch.inverse(Q),U_) * S\n",
        "    return U, V_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cOQl3NMhJOIk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import copy\n",
        "w=copy.deepcopy(model.word_embeddings.weight.data)\n",
        "# del model\n",
        "#del raw_data ,train_data, valid_data, test_data, word_to_id, id_2_word \n",
        "# torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tjbl4qEF3BJJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del U, V\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QvWjicSPzTDz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "VANILA SVD "
      ]
    },
    {
      "metadata": {
        "id": "UbolIj7yvrQS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d370caa-45a1-489d-83ef-cb36e1ad94c6"
      },
      "cell_type": "code",
      "source": [
        "u, s, v = torch.svd(w)\n",
        "ns = torch.cat((s[:40], torch.zeros_like(s[40:])))\n",
        "new_weight = torch.matmul(u*ns, v.t()) \n",
        "model.state_dict()['word_embeddings.weight'].data.copy_(new_weight)\n",
        "print('Test Perplexity: {:8.2f}'.format(run_epoch(model, test_data)))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Perplexity:   149.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X6q2I_MJzWkb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Weighted SVD"
      ]
    },
    {
      "metadata": {
        "id": "tk8fblGB6FHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2907af82-01c2-4429-e0fc-7ce770876c49"
      },
      "cell_type": "code",
      "source": [
        "U , V = weighted_svd(w, freq_sqr,rank_to_be = 40)\n",
        "new_weight = torch.matmul(U, V.t()) \n",
        "model.state_dict()['word_embeddings.weight'].data.copy_(new_weight)\n",
        "print('Test Perplexity: {:8.2f}'.format(run_epoch(model, test_data)))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Perplexity:   150.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nJFv_7EbRgsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02b23deb-edd9-4b05-d98d-6e3ed0880790"
      },
      "cell_type": "code",
      "source": [
        "#rank_to_be = 40\n",
        "number_of_clusters = 5 \n",
        "rank_to_be_on_each_cluster = 40\n",
        "clusters = list(torch.split(w, int(w.shape[0] / number_of_clusters)))\n",
        "freq_cls = np.array_split(freq_sqr, number_of_clusters)\n",
        "V = {}\n",
        "U = {}\n",
        "for p in range(number_of_clusters): \n",
        "    U[p], V[p]  = weighted_svd(clusters[p],freq_cls[p], rank_to_be_on_each_cluster)  \n",
        "new_weight = torch.cat([torch.matmul(U[p], V[p].t()) for p in range(number_of_clusters)])\n",
        "model.state_dict()['word_embeddings.weight'].data.copy_(new_weight)\n",
        "print('Test Perplexity: {:8.2f}'.format(run_epoch(model, test_data)))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Perplexity:   150.51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Os4xMHcn4Uoq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_ranks(freq_cls, rank_min):\n",
        "        ranks = [sum(i)/len(i) for i in freq_cls] \n",
        "        ranks /= ranks[-1] \n",
        "        ranks *=rank_min \n",
        "        ranks = {i : int(ranks[i]) for i in range(len(ranks))}\n",
        "        return ranks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MloMDqNVxIAO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19f12a62-95a3-4e24-e11c-1d0e16aaac46"
      },
      "cell_type": "code",
      "source": [
        "rank_min = 40 \n",
        "number_of_clusters = 5 \n",
        "clusters = list(torch.split(w, int(w.shape[0] / number_of_clusters)))\n",
        "freq_cls = np.array_split(freq_sqr, number_of_clusters)\n",
        "V = {}\n",
        "U = {}\n",
        "cluster_ranks = find_ranks(freq_cls, rank_min)\n",
        " \n",
        "for p in range(number_of_clusters): \n",
        "    U[p], V[p]  = weighted_svd(clusters[p],freq_cls[p], int(cluster_ranks[p]))  \n",
        "    \n",
        "new_weight = torch.cat([torch.matmul(U[p], V[p].t()) for p in range(number_of_clusters)])\n",
        "model.state_dict()['word_embeddings.weight'].data.copy_(new_weight)\n",
        "print('Test Perplexity: {:8.2f}'.format(run_epoch(model, test_data)))\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Perplexity:   146.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kUtZR8H9N0Tn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save( 'new_weights.npy', new_weights)\n",
        "np.save( 'ranks.npy', np.array(ranks))\n",
        "np.save( 'word_in_cls.npy', np.array(word_in_cls))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vmC_XNVK5rlq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "without rank recalculation every time"
      ]
    },
    {
      "metadata": {
        "id": "TiXiARxY5v6O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def group_reduce(A, c, r, t_max, m_min):\n",
        "    #init clusters\n",
        "    def find_num(i):\n",
        "        for j in range(c):\n",
        "            i -= clusters_len[j]\n",
        "            if i < 0:\n",
        "                return j, clusters_len[j] + i\n",
        "        \n",
        "        \n",
        "        return 0, 0\n",
        "    \n",
        "    \n",
        "    def find_ranks(freq_cls, rank_min):\n",
        "        ranks = [sum(i)/len(i) for i in freq_cls] \n",
        "        ranks /= ranks[-1] \n",
        "        ranks *=rank_min \n",
        "        ranks = {i : int(ranks[i]) for i in range(len(ranks))}\n",
        "        return ranks\n",
        "    \n",
        "    clusters_len = []\n",
        "    clusters = list(torch.split(A, int(A.shape[0] / c)))\n",
        "    for cluster in clusters:\n",
        "        clusters_len.append(cluster.shape[0])\n",
        "    words_in_cls = np.array_split(list(id_2_word.keys()),c)\n",
        "    freq_cls = np.array_split(freq_sqr, c)\n",
        "    ranks = find_ranks(freq_cls, r)\n",
        "    V = {}\n",
        "    U = {}\n",
        "    V_Vt = []\n",
        "\n",
        "    for p in range(c): \n",
        "        U[p], V[p]  = weighted_svd(clusters[p],freq_cls[p], ranks[p])\n",
        "        ranks[p] = torch.matrix_rank(torch.matmul(U[p], V[p].t()))\n",
        "    for t in range(t_max):\n",
        "        M = {}\n",
        "        Err = []\n",
        "        for p in range(c):\n",
        "            error_p = torch.sqrt((A - torch.matmul(A, torch.matmul(V[p], V[p].t()))).pow(2).sum(1))\n",
        "            Err.append(error_p)\n",
        "        Err = torch.stack(Err, dim = 0)\n",
        "        e, g = torch.min(Err, dim = 0)\n",
        "        del Err\n",
        "        del error_p\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        for i in range(A.shape[0]):\n",
        "            if g[i] !=  find_num(i)[0]:\n",
        "                M[i] = (e[i], g[i])\n",
        "                \n",
        "        m = 0.1*len(M)\n",
        "        t = 0\n",
        "        is_changed = [False]*c\n",
        "        for key, value in sorted(M.items(), key=lambda kv: kv[1]):\n",
        "            \n",
        "            words_in_cls[value[1]] = np.append(words_in_cls[value[1]], words_in_cls[find_num(key)[0]][find_num(key)[1]])\n",
        "            words_in_cls[find_num(key)[0]] = np.delete(words_in_cls[find_num(key)[0]], find_num(key)[1])\n",
        "            \n",
        "            freq_cls[value[1]] = np.append(freq_cls[value[1]], freq_cls[find_num(key)[0]][find_num(key)[1]])\n",
        "            freq_cls[find_num(key)[0]] = np.delete(freq_cls[find_num(key)[0]], find_num(key)[1])\n",
        "\n",
        "            clusters[value[1]] = torch.cat((clusters[value[1]],torch.unsqueeze(clusters[find_num(key)[0]][find_num(key)[1]],dim = 0)))\n",
        "            clusters[find_num(key)[0]] = torch.cat((clusters[find_num(key)[0]][ :find_num(key)[1]], \n",
        "                                                     clusters[find_num(key)[0]][find_num(key)[1] + 1: ]), dim = 0)\n",
        "\n",
        "            clusters_len[find_num(key)[0]] -= 1\n",
        "            clusters_len[value[1]] += 1\n",
        "            is_changed[value[1]] = True\n",
        "            is_changed[find_num(key)[0]] = True\n",
        "#             print(\"Hooray\")\n",
        "            t += 1\n",
        "            if m == t:\n",
        "                break;\n",
        "        if m < m_min:\n",
        "            return [torch.matmul(U[p], V[p].t()) for p in range(c)] ,ranks, words_in_cls\n",
        "#        ranks= find_ranks(freq_cls, r)\n",
        "        for p in range(c):\n",
        "            if is_changed[p]:\n",
        "                \n",
        "                U[p], V[p]  = weighted_svd(clusters[p],freq_cls[p], ranks[p] )\n",
        "                #ranks[p] = torch.matrix_rank(torch.matmul(U[p], V[p].t()))\n",
        "                \n",
        "    return [torch.matmul(U[p], V[p].t()) for p in range(c)], ranks, words_in_cls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2QlpnL-q52IL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "64f1391b-9d26-466b-b1a3-84e6aff677a9"
      },
      "cell_type": "code",
      "source": [
        "new_weights , ranks , word_in_cl = group_reduce(w, c = 5, r = 40, t_max = 100, m_min = 10)\n",
        "nw = torch.empty_like(w)\n",
        "for i in range(10000):\n",
        "    idx =np.where(np.concatenate(word_in_cls)==i)\n",
        "    nw[i]=torch.cat(new_weights)[idx[0]]\n",
        "model.state_dict()['word_embeddings.weight'].data.copy_(nw)\n",
        "print('Test Perplexity: {:8.2f}'.format(run_epoch(model, test_data)))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([8014]),)\n",
            "Test Perplexity:   148.34\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}