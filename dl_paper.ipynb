{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_paper.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "NFdz5c7CxNyq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "import torch\n",
        "import torch.nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from lm import repackage_hidden, LM_LSTM\n",
        "import reader\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oQ2Z7nR3mBVY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ниже - обучение базовой модели из статьи: bigLSTM(small PTB)\n",
        "Обученная модель сохранена в файле lm_model.pt  \n",
        "Поэтому это можно скипнуть"
      ]
    },
    {
      "metadata": {
        "id": "4prWa3UzyNGg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "data = 'data'\n",
        "hidden_size = 1500\n",
        "num_steps = 35\n",
        "num_layers = 2\n",
        "batch_size = 20\n",
        "num_epochs = 13 \n",
        "dp_keep_prob = 0.35\n",
        "inital_lr = 20.0\n",
        "save = 'lm_model.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oOZhDYxL6mAT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
        "  \n",
        "\n",
        "  if type(h) == torch.Tensor:\n",
        "    return Variable(h.data)\n",
        "  else:\n",
        "    d = tuple(repackage_hidden(v) for v in h)\n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xK-sZhMs2Tq8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_epoch(model, data, is_train=False, lr=1.0):\n",
        "    \"\"\"Runs the model on the given data.\"\"\"\n",
        "    if is_train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    epoch_size = ((len(data) // model.batch_size) - 1) // model.num_steps\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden()\n",
        "    hidden[0].requires_grad=True\n",
        "    hidden[1].requires_grad=True\n",
        "    costs = 0.0\n",
        "    iters = 0\n",
        "    for step, (x, y) in enumerate(reader.ptb_iterator(data, model.batch_size, model.num_steps)):\n",
        "        inputs =torch.from_numpy(x.astype(np.int64)).transpose(0, 1).contiguous().cuda()\n",
        "        model.zero_grad()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        targets = torch.from_numpy(y.astype(np.int64)).transpose(0, 1).contiguous().cuda()\n",
        "        tt = torch.squeeze(targets.view(-1, model.batch_size * model.num_steps))\n",
        "\n",
        "        loss = criterion(outputs.view(-1, model.vocab_size), tt)\n",
        "        #print( loss.data.item() , model.num_steps)\n",
        "        costs += loss.data.item() * model.num_steps\n",
        "        iters += model.num_steps\n",
        "\n",
        "        if is_train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)\n",
        "            for p in model.parameters():\n",
        "                p.data.add_(-lr, p.grad.data)\n",
        "            if step % (epoch_size // 10) == 10:\n",
        "                print(\"{} perplexity: {:8.2f} speed: {} wps\".format(step * 1.0 / epoch_size, np.exp(costs / iters),\n",
        "                                                       iters * model.batch_size / (time.time() - start_time)))\n",
        "    return np.exp(costs / iters)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPpy_FSz0n3V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for google collab\n",
        "!mkdir /content/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tINPnduTz_tI",
        "colab_type": "code",
        "outputId": "f7abc1b5-cab0-45ea-9a4b-f8f4d1c49475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2758
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "raw_data = reader.ptb_raw_data(data_path=data)\n",
        "train_data, valid_data, test_data, word_to_id, id_2_word = raw_data\n",
        "vocab_size = len(word_to_id)\n",
        "print('Vocabluary size: {}'.format(vocab_size))\n",
        "model = LM_LSTM(embedding_dim=hidden_size, num_steps=num_steps, batch_size=batch_size,\n",
        "                vocab_size=vocab_size, num_layers=num_layers, dp_keep_prob=dp_keep_prob)\n",
        "model.cuda()\n",
        "lr = inital_lr\n",
        "# decay factor for learning rate\n",
        "lr_decay_base = 1 / 1.15\n",
        "# we will not touch lr for the first m_flat_lr epochs\n",
        "m_flat_lr = 14.0\n",
        "\n",
        "print(\"########## Training ##########################\")\n",
        "for epoch in range(num_epochs):\n",
        "    lr_decay = lr_decay_base ** max(epoch - m_flat_lr, 0)\n",
        "    lr = lr * lr_decay # decay lr if it is time\n",
        "    train_p = run_epoch(model, train_data, True, lr)\n",
        "    print('Train perplexity at epoch {}: {:8.2f}'.format(epoch, train_p))\n",
        "    print('Validation perplexity at epoch {}: {:8.2f}'.format(epoch, run_epoch(model, valid_data)))\n",
        "print(\"########## Testing ##########################\")\n",
        "model.batch_size = 1 # to make sure we process all the data\n",
        "print('Test Perplexity: {:8.2f}'.format(run_epoch(model, test_data)))\n",
        "with open(save, 'wb') as f:\n",
        "    torch.save(model, f)\n",
        "print(\"########## Done! ##########################\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabluary size: 10000\n",
            "########## Training ##########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.007535795026375283 perplexity: 11527.40 speed: 2844.6852155846427 wps\n",
            "0.10700828937452901 perplexity:  1499.09 speed: 3110.6207065535705 wps\n",
            "0.20648078372268275 perplexity:   990.26 speed: 3120.316663844005 wps\n",
            "0.3059532780708365 perplexity:   770.46 speed: 3121.5829530410356 wps\n",
            "0.4054257724189902 perplexity:   656.31 speed: 3121.103484107114 wps\n",
            "0.5048982667671439 perplexity:   582.19 speed: 3120.606021969607 wps\n",
            "0.6043707611152976 perplexity:   522.02 speed: 3120.8393492188093 wps\n",
            "0.7038432554634514 perplexity:   479.49 speed: 3120.3037921147547 wps\n",
            "0.8033157498116051 perplexity:   446.33 speed: 3120.6308103242695 wps\n",
            "0.9027882441597589 perplexity:   417.47 speed: 3120.3670190784806 wps\n",
            "Train perplexity at epoch 0:   395.04\n",
            "Validation perplexity at epoch 0:   225.94\n",
            "0.007535795026375283 perplexity:   278.42 speed: 3054.9332994318183 wps\n",
            "0.10700828937452901 perplexity:   220.00 speed: 3123.216264339193 wps\n",
            "0.20648078372268275 perplexity:   227.16 speed: 3126.2871508714006 wps\n",
            "0.3059532780708365 perplexity:   220.70 speed: 3128.5879420107217 wps\n",
            "0.4054257724189902 perplexity:   218.11 speed: 3129.9416316879556 wps\n",
            "0.5048982667671439 perplexity:   215.18 speed: 3130.4674294878378 wps\n",
            "0.6043707611152976 perplexity:   209.38 speed: 3130.120141220855 wps\n",
            "0.7038432554634514 perplexity:   206.10 speed: 3130.327986782227 wps\n",
            "0.8033157498116051 perplexity:   203.06 speed: 3130.581375278747 wps\n",
            "0.9027882441597589 perplexity:   198.68 speed: 3130.621902601051 wps\n",
            "Train perplexity at epoch 1:   195.59\n",
            "Validation perplexity at epoch 1:   163.99\n",
            "0.007535795026375283 perplexity:   204.84 speed: 3045.3155422907676 wps\n",
            "0.10700828937452901 perplexity:   158.84 speed: 3121.689470771744 wps\n",
            "0.20648078372268275 perplexity:   168.29 speed: 3126.926816471369 wps\n",
            "0.3059532780708365 perplexity:   164.64 speed: 3128.0770375321954 wps\n",
            "0.4054257724189902 perplexity:   164.26 speed: 3128.7476983054053 wps\n",
            "0.5048982667671439 perplexity:   163.37 speed: 3129.1421923288144 wps\n",
            "0.6043707611152976 perplexity:   160.13 speed: 3129.622283629949 wps\n",
            "0.7038432554634514 perplexity:   158.86 speed: 3129.9602513678365 wps\n",
            "0.8033157498116051 perplexity:   157.65 speed: 3130.4437613159575 wps\n",
            "0.9027882441597589 perplexity:   154.81 speed: 3130.435924941554 wps\n",
            "Train perplexity at epoch 2:   153.24\n",
            "Validation perplexity at epoch 2:   136.77\n",
            "0.007535795026375283 perplexity:   168.38 speed: 3044.318585005081 wps\n",
            "0.10700828937452901 perplexity:   130.38 speed: 3116.014752600983 wps\n",
            "0.20648078372268275 perplexity:   140.39 speed: 3118.011590039568 wps\n",
            "0.3059532780708365 perplexity:   137.27 speed: 3120.2087827084465 wps\n",
            "0.4054257724189902 perplexity:   137.27 speed: 3120.5881555704686 wps\n",
            "0.5048982667671439 perplexity:   136.75 speed: 3121.2186241090512 wps\n",
            "0.6043707611152976 perplexity:   134.30 speed: 3121.4727641491654 wps\n",
            "0.7038432554634514 perplexity:   133.70 speed: 3120.838544911427 wps\n",
            "0.8033157498116051 perplexity:   133.05 speed: 3121.4918143540754 wps\n",
            "0.9027882441597589 perplexity:   130.80 speed: 3121.754702721638 wps\n",
            "Train perplexity at epoch 3:   129.81\n",
            "Validation perplexity at epoch 3:   121.43\n",
            "0.007535795026375283 perplexity:   142.38 speed: 3056.9661106646063 wps\n",
            "0.10700828937452901 perplexity:   112.14 speed: 3116.6365099702457 wps\n",
            "0.20648078372268275 perplexity:   121.89 speed: 3118.5606748810887 wps\n",
            "0.3059532780708365 perplexity:   119.05 speed: 3119.941687428903 wps\n",
            "0.4054257724189902 perplexity:   119.19 speed: 3120.195066666644 wps\n",
            "0.5048982667671439 perplexity:   118.91 speed: 3120.1255942098232 wps\n",
            "0.6043707611152976 perplexity:   116.89 speed: 3120.4484689229175 wps\n",
            "0.7038432554634514 perplexity:   116.61 speed: 3120.646536540334 wps\n",
            "0.8033157498116051 perplexity:   116.33 speed: 3120.8276208868187 wps\n",
            "0.9027882441597589 perplexity:   114.37 speed: 3120.993297021377 wps\n",
            "Train perplexity at epoch 4:   113.65\n",
            "Validation perplexity at epoch 4:   111.71\n",
            "0.007535795026375283 perplexity:   126.70 speed: 3047.1702464030604 wps\n",
            "0.10700828937452901 perplexity:    99.00 speed: 3126.7410523764897 wps\n",
            "0.20648078372268275 perplexity:   108.58 speed: 3130.2008905247017 wps\n",
            "0.3059532780708365 perplexity:   106.08 speed: 3131.227919860083 wps\n",
            "0.4054257724189902 perplexity:   106.24 speed: 3131.6185519596866 wps\n",
            "0.5048982667671439 perplexity:   106.11 speed: 3131.097838690076 wps\n",
            "0.6043707611152976 perplexity:   104.39 speed: 3131.0518198578334 wps\n",
            "0.7038432554634514 perplexity:   104.33 speed: 3131.0049610386013 wps\n",
            "0.8033157498116051 perplexity:   104.29 speed: 3131.2887342501626 wps\n",
            "0.9027882441597589 perplexity:   102.61 speed: 3130.973484237197 wps\n",
            "Train perplexity at epoch 5:   102.13\n",
            "Validation perplexity at epoch 5:   104.43\n",
            "0.007535795026375283 perplexity:   114.80 speed: 3074.614756144395 wps\n",
            "0.10700828937452901 perplexity:    89.30 speed: 3125.5356192019467 wps\n",
            "0.20648078372268275 perplexity:    98.40 speed: 3128.337797698073 wps\n",
            "0.3059532780708365 perplexity:    96.27 speed: 3128.576367971876 wps\n",
            "0.4054257724189902 perplexity:    96.55 speed: 3129.2684543287314 wps\n",
            "0.5048982667671439 perplexity:    96.40 speed: 3129.190174962964 wps\n",
            "0.6043707611152976 perplexity:    94.94 speed: 3129.369424539029 wps\n",
            "0.7038432554634514 perplexity:    94.98 speed: 3129.6996271006674 wps\n",
            "0.8033157498116051 perplexity:    95.01 speed: 3129.68969585116 wps\n",
            "0.9027882441597589 perplexity:    93.61 speed: 3130.01270240636 wps\n",
            "Train perplexity at epoch 6:    93.21\n",
            "Validation perplexity at epoch 6:    99.44\n",
            "0.007535795026375283 perplexity:   104.83 speed: 3052.9926542647418 wps\n",
            "0.10700828937452901 perplexity:    82.16 speed: 3116.444451134825 wps\n",
            "0.20648078372268275 perplexity:    90.81 speed: 3118.4755409980125 wps\n",
            "0.3059532780708365 perplexity:    88.76 speed: 3118.7239474297107 wps\n",
            "0.4054257724189902 perplexity:    89.08 speed: 3119.559675361045 wps\n",
            "0.5048982667671439 perplexity:    88.92 speed: 3120.0974374800444 wps\n",
            "0.6043707611152976 perplexity:    87.56 speed: 3120.3138087020716 wps\n",
            "0.7038432554634514 perplexity:    87.69 speed: 3120.5506157068835 wps\n",
            "0.8033157498116051 perplexity:    87.71 speed: 3120.7628125445704 wps\n",
            "0.9027882441597589 perplexity:    86.41 speed: 3121.536123863961 wps\n",
            "Train perplexity at epoch 7:    86.07\n",
            "Validation perplexity at epoch 7:    96.39\n",
            "0.007535795026375283 perplexity:    97.95 speed: 3051.147576419123 wps\n",
            "0.10700828937452901 perplexity:    76.40 speed: 3118.4900687619725 wps\n",
            "0.20648078372268275 perplexity:    84.41 speed: 3121.029011380527 wps\n",
            "0.3059532780708365 perplexity:    82.60 speed: 3120.1187981165494 wps\n",
            "0.4054257724189902 perplexity:    82.82 speed: 3118.5381473758557 wps\n",
            "0.5048982667671439 perplexity:    82.79 speed: 3118.943404504771 wps\n",
            "0.6043707611152976 perplexity:    81.60 speed: 3118.6199440172904 wps\n",
            "0.7038432554634514 perplexity:    81.74 speed: 3118.9393288403267 wps\n",
            "0.8033157498116051 perplexity:    81.80 speed: 3119.028332393275 wps\n",
            "0.9027882441597589 perplexity:    80.62 speed: 3119.2901530384406 wps\n",
            "Train perplexity at epoch 8:    80.34\n",
            "Validation perplexity at epoch 8:    92.68\n",
            "0.007535795026375283 perplexity:    92.88 speed: 3050.59105888656 wps\n",
            "0.10700828937452901 perplexity:    71.66 speed: 3125.378802242401 wps\n",
            "0.20648078372268275 perplexity:    79.32 speed: 3128.730335415906 wps\n",
            "0.3059532780708365 perplexity:    77.31 speed: 3131.0652073469296 wps\n",
            "0.4054257724189902 perplexity:    77.49 speed: 3131.1463642457165 wps\n",
            "0.5048982667671439 perplexity:    77.46 speed: 3131.9011913372665 wps\n",
            "0.6043707611152976 perplexity:    76.34 speed: 3131.660102013595 wps\n",
            "0.7038432554634514 perplexity:    76.59 speed: 3131.12413206841 wps\n",
            "0.8033157498116051 perplexity:    76.73 speed: 3131.11460833169 wps\n",
            "0.9027882441597589 perplexity:    75.61 speed: 3131.3989889082723 wps\n",
            "Train perplexity at epoch 9:    75.36\n",
            "Validation perplexity at epoch 9:    90.74\n",
            "0.007535795026375283 perplexity:    86.56 speed: 3065.331907862766 wps\n",
            "0.10700828937452901 perplexity:    67.38 speed: 3126.429869573797 wps\n",
            "0.20648078372268275 perplexity:    74.75 speed: 3127.4196023195373 wps\n",
            "0.3059532780708365 perplexity:    72.93 speed: 3129.925176420097 wps\n",
            "0.4054257724189902 perplexity:    73.06 speed: 3130.242147436159 wps\n",
            "0.5048982667671439 perplexity:    73.08 speed: 3130.823167069332 wps\n",
            "0.6043707611152976 perplexity:    72.14 speed: 3130.660992207903 wps\n",
            "0.7038432554634514 perplexity:    72.36 speed: 3130.3486687326554 wps\n",
            "0.8033157498116051 perplexity:    72.54 speed: 3130.430851438321 wps\n",
            "0.9027882441597589 perplexity:    71.49 speed: 3131.0320953520786 wps\n",
            "Train perplexity at epoch 10:    71.27\n",
            "Validation perplexity at epoch 10:    88.63\n",
            "0.007535795026375283 perplexity:    82.20 speed: 3027.6217532962132 wps\n",
            "0.10700828937452901 perplexity:    63.60 speed: 3112.7815242711204 wps\n",
            "0.20648078372268275 perplexity:    70.70 speed: 3116.084958972023 wps\n",
            "0.3059532780708365 perplexity:    69.06 speed: 3117.498975649682 wps\n",
            "0.4054257724189902 perplexity:    69.17 speed: 3118.8664301526483 wps\n",
            "0.5048982667671439 perplexity:    69.14 speed: 3118.774387432358 wps\n",
            "0.6043707611152976 perplexity:    68.19 speed: 3118.8379751694083 wps\n",
            "0.7038432554634514 perplexity:    68.48 speed: 3119.205567816523 wps\n",
            "0.8033157498116051 perplexity:    68.66 speed: 3119.5799877393256 wps\n",
            "0.9027882441597589 perplexity:    67.70 speed: 3120.1113615518725 wps\n",
            "Train perplexity at epoch 11:    67.46\n",
            "Validation perplexity at epoch 11:    87.20\n",
            "0.007535795026375283 perplexity:    77.51 speed: 3052.1827604593223 wps\n",
            "0.10700828937452901 perplexity:    60.81 speed: 3116.7413865845747 wps\n",
            "0.20648078372268275 perplexity:    67.27 speed: 3118.212640582637 wps\n",
            "0.3059532780708365 perplexity:    65.64 speed: 3117.7056696759023 wps\n",
            "0.4054257724189902 perplexity:    65.74 speed: 3117.9665079703245 wps\n",
            "0.5048982667671439 perplexity:    65.72 speed: 3117.700060649305 wps\n",
            "0.6043707611152976 perplexity:    64.88 speed: 3117.563265960668 wps\n",
            "0.7038432554634514 perplexity:    65.14 speed: 3117.3702662433093 wps\n",
            "0.8033157498116051 perplexity:    65.32 speed: 3117.4761582273204 wps\n",
            "0.9027882441597589 perplexity:    64.40 speed: 3117.988628233816 wps\n",
            "Train perplexity at epoch 12:    64.23\n",
            "Validation perplexity at epoch 12:    86.27\n",
            "########## Testing ##########################\n",
            "Test Perplexity:    84.63\n",
            "########## Done! ##########################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WG-RPKlZmtnM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Загрузка обученной модели"
      ]
    },
    {
      "metadata": {
        "id": "thTFxgR2T8jD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "4d89eb94-2a0f-4f6a-a19a-cd5e81f1a576"
      },
      "cell_type": "code",
      "source": [
        "model =  torch.load('./lm_model.pt')\n",
        "model.cuda()\n",
        "model.eval()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LM_LSTM(\n",
              "  (dropout): Dropout(p=0.65)\n",
              "  (word_embeddings): Embedding(10000, 1500)\n",
              "  (lstm): LSTM(1500, 1500, num_layers=2, dropout=0.65)\n",
              "  (sm_fc): Linear(in_features=1500, out_features=10000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "zOV2GEOjhKn_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4c13587-8a33-4fcc-dd95-0773f21afacf"
      },
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "929589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}